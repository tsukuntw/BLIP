'''
 * Copyright (c) 2022, salesforce.com, inc.
 * All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause
 * By Junnan Li
'''
import warnings
warnings.filterwarnings("ignore")

from models.vit import VisionTransformer, interpolate_pos_embed, Block
from models.med import BertConfig, BertModel, BertLMHeadModel
from transformers import BertTokenizer

import torch
from torch import nn
import torch.nn.functional as F

import os
from urllib.parse import urlparse
from timm.models.hub import download_cached_file
from .swin_transformer_necks import SwinTransformerNeck
from .swin_transformer_scale import SwinTransformerScale

from mmengine.model.weight_init import (constant_init, trunc_normal_,
                                        trunc_normal_init)
 
from collections import OrderedDict
# from transformers import ViTConfig, ViTModel, BaseModelOutputWithPooling
from functools import partial
import numpy as np
#
import math
from mmcv.cnn import ConvModule
from models.backbones.swin import SwinTransformer

class BLIP_Base(nn.Module):
    def __init__(self,                 
                 med_config = 'configs/med_config.json',  
                 image_size = 224,
                 vit = 'base',
                 vit_grad_ckpt = False,
                 vit_ckpt_layer = 0,  
                 use_swin = False,
                 use_contrastive = False,
                 more_output = True, 
                 use_vit_layers = 1.0,          
                 ):
        """
        Args:
            med_config (str): path for the mixture of encoder-decoder model's configuration file
            image_size (int): input image size
            vit (str): model size of vision transformer
        """               
        super().__init__()
        
        #
        self.use_swin = use_swin
        
        if self.use_swin:
            # 0910
            assert vit in ['scale', 'local', 'base'], "mode parameter must be scale, local, or base"
            self.visual_encoder = Create_Vit_Scale(vit,image_size, vit_grad_ckpt, vit_ckpt_layer, 
                                             remain_depth_ratio = use_vit_layers, more_output = more_output)
            vision_width = self.visual_encoder.vision_width
        else:
            self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)
        
        self.tokenizer = init_tokenizer()   
        med_config = BertConfig.from_json_file(med_config)
        med_config.encoder_width = vision_width
        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)  
        
        # 
        self.text_decoder = BertLMHeadModel(config=med_config)
        text_width = self.text_decoder.config.hidden_size

        
    def forward(self, image, caption, mode):
        
        assert mode in ['image', 'text', 'multimodal'], "mode parameter must be image, text, or multimodal"
        text = self.tokenizer(caption, return_tensors="pt").to(image.device) 
        
        if mode=='image':    
            # return image features
            image_embeds = self.visual_encoder(image)   
            if not self.use_swin:          
                return image_embeds
            else:
                return image_embeds[:,1:17,:], image_embeds[:,17:,:], 
        
        elif mode=='text':
            # return text features
            text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      
                                            return_dict = True, mode = 'text')  
            return text_output.last_hidden_state
        
        elif mode=='multimodal':
            # from decoder
            image_embeds = self.visual_encoder(image)
            image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)
            
            text = self.tokenizer(caption, padding='longest', truncation=True, max_length=40, return_tensors="pt").to(image.device) 
            
            print('text.input_ids {}'.format(text.input_ids))
            
            text.input_ids[:,0] = self.tokenizer.enc_token_id
            
            print('text.input_ids {}'.format(text.input_ids))
            
            decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100)         
            decoder_targets[:,:1] = -100
         
            decoder_output = self.text_decoder(text.input_ids, 
                                               attention_mask = text.attention_mask, 
                                               encoder_hidden_states = image_embeds, # image_embeds
                                               encoder_attention_mask = image_atts,
                                               output_hidden_states = True,
                                               labels = decoder_targets,
                                               return_dict = True,   
                                              ) 
            return decoder_output.hidden_states
        
        
            # blip baseline
            # return multimodel features
            # image_embeds = self.visual_encoder(image)    
            # image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)      
            
            # text.input_ids[:,0] = self.tokenizer.enc_token_id
            # output = self.text_encoder(text.input_ids,
            #                            attention_mask = text.attention_mask,
            #                            encoder_hidden_states = image_embeds,
            #                            encoder_attention_mask = image_atts,      
            #                            return_dict = True,
            #                           )              
            # return output.last_hidden_state
        
        
        
class BLIP_Decoder(nn.Module):
    def __init__(self,                 
                 med_config = 'configs/med_config.json',  
                 image_size = 384,
                 vit = 'base',
                 vit_grad_ckpt = False,
                 vit_ckpt_layer = 0,
                 prompt = 'a picture of ',
                 use_swin = False,
                 use_contrastive = False,
                 more_output = True,
                 proj_dim = 256,
                 use_vit_layers = 1.0,
                 scst=False,
                 scst_config=False,
                 ):
        """
        Args:
            med_config (str): path for the mixture of encoder-decoder model's configuration file
            image_size (int): input image size
            vit (str): model size of vision transformer
        """            
        super().__init__()
        # self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer,)
        
        #
        self.use_swin = use_swin
        self.use_contrastive = use_contrastive
        self.image_size = image_size
        if self.use_swin:
            # 0910
            self.visual_encoder = Create_Vit_Scale(vit, image_size)
            vision_width = self.visual_encoder.vision_width
        self.more_output = more_output
        
        self.tokenizer = init_tokenizer()   
        med_config = BertConfig.from_json_file(med_config)
        med_config.encoder_width = vision_width
        self.text_decoder = BertLMHeadModel(config=med_config)
        text_width = self.text_decoder.config.hidden_size
        
        self.prompt = prompt
        self.prompt_length = len(self.tokenizer(self.prompt).input_ids)-1
        
        
    def forward(self, image, caption, is_train=True, out_size=5, scst=False, do_sample=False):
       
        image_embeds = self.visual_encoder(image)
        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)
        
        text = self.tokenizer(caption, padding='longest', truncation=True, max_length=40, return_tensors="pt").to(image.device) 
        
        text.input_ids[:,0] = self.tokenizer.bos_token_id
        
        decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100)         
        decoder_targets[:,:self.prompt_length] = -100
     
        decoder_output = self.text_decoder(text.input_ids, 
                                           attention_mask = text.attention_mask, 
                                           encoder_hidden_states = image_embeds, # image_embeds
                                           encoder_attention_mask = image_atts,
                                           output_hidden_states = True,
                                           labels = decoder_targets,
                                           return_dict = True,   
                                          )   
        loss_lm = decoder_output.loss
        
        return loss_lm
        
    def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0):
        image_embeds = self.visual_encoder(image)
        if not sample:
            image_embeds = image_embeds.repeat_interleave(num_beams,dim=0)
            
        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)
        model_kwargs = {"encoder_hidden_states": image_embeds, "encoder_attention_mask":image_atts}
        
        prompt = [self.prompt] * image.size(0)
        input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids.to(image.device) 
        input_ids[:,0] = self.tokenizer.bos_token_id
        input_ids = input_ids[:, :-1] 

        if sample:
            #nucleus sampling
            outputs = self.text_decoder.generate(input_ids=input_ids,
                                                  max_length=max_length,
                                                  min_length=min_length,
                                                  do_sample=True,
                                                  top_p=top_p,
                                                  num_return_sequences=1,
                                                  eos_token_id=self.tokenizer.sep_token_id,
                                                  pad_token_id=self.tokenizer.pad_token_id, 
                                                  repetition_penalty=1.1,                                            
                                                  **model_kwargs)
        else:
            #beam search
            outputs = self.text_decoder.generate(input_ids=input_ids,
                                                  max_length=max_length,
                                                  min_length=min_length,
                                                  num_beams=num_beams,
                                                  eos_token_id=self.tokenizer.sep_token_id,
                                                  pad_token_id=self.tokenizer.pad_token_id,     
                                                  repetition_penalty=repetition_penalty,
                                                  **model_kwargs)
                                                  
        captions = []    
        for output in outputs:
            caption = self.tokenizer.decode(output, skip_special_tokens=True)    
            captions.append(caption[len(self.prompt):])
        return captions

def blip_decoder(pretrained='',**kwargs):
    model = BLIP_Decoder(**kwargs)
    if pretrained:
        model,msg = load_checkpoint(model,pretrained)
        print('missing keys: {}'.format(msg.missing_keys))
        # assert(len(msg.missing_keys)==0)
    return model    
    
def blip_feature_extractor(pretrained='', scale_pretrained = '',**kwargs):
    model = BLIP_Base(**kwargs)
    if pretrained:
        model,msg = load_checkpoint(model,pretrained)
        print('missing keys: {}'.format(len(msg.missing_keys)))
        # assert(len(msg.missing_keys)==0)
    return model        

def init_tokenizer():
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    tokenizer.add_special_tokens({'bos_token':'[DEC]'})
    tokenizer.add_special_tokens({'additional_special_tokens':['[ENC]']})       
    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]  
    return tokenizer


def create_vit(vit, image_size, use_grad_checkpointing=False, ckpt_layer=0, drop_path_rate=0):
    assert vit in ['base', 'large'], "vit parameter must be base or large"
    if vit=='base':
        vision_width = 768
        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12, 
                                           num_heads=12, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,
                                           drop_path_rate=0 or drop_path_rate
                                          )   
    elif vit=='large':
        vision_width = 1024
        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=24, 
                                           num_heads=16, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,
                                           drop_path_rate=0.1 or drop_path_rate
                                          )
        
    return visual_encoder, vision_width


class Create_Vit_Scale(nn.Module):
    def __init__(self,vit, image_size, use_grad_checkpointing=False, ckpt_layer=0, drop_path_rate=0,
                 remain_depth_ratio = 1.0, more_output = False, use_scale_swin = False, use_scale_local = False, scale_emb = False,
                 use_scale_vit = False, use_scale_merging = False, relative_pos = False):
        super(Create_Vit_Scale, self).__init__()
        assert vit in ['scale', 'local', 'merging', 'large'], "vit parameter must be base or large"
        # 
        self.image_size = image_size

        if vit=='scale' or vit=='local' or vit=='merging':
            if vit == 'scale':
                use_scale_swin = True
            elif vit == 'local':
                use_scale_local = True
                print('enable local vit, image_size {}'.format(image_size))
                from .localvit import LocalTransformerScale
            elif vit == 'merging':
                use_scale_merging = True
                from .swin_transformer_scale import SwinPatchMerging
                print('enable swin merging, image_size {}'.format(image_size))
            vision_width = 768
            visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12, 
                                               num_heads=12, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,
                                               drop_path_rate=0 or drop_path_rate
                                               )
        # shiyan large 1021                                     
        elif vit=='large':
            vision_width = 1024
            visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=24, 
                                               num_heads=16, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,
                                               drop_path_rate=0.1 or drop_path_rate
                                              )
            use_scale_swin = True
        self.vision_width = vision_width
        self.visual_encoder = visual_encoder
        self.scale_emb = scale_emb
        self.use_scale_vit = use_scale_vit
        self.use_scale_swin = use_scale_swin
        self.use_scale_local = use_scale_local
        self.use_scale_merging = use_scale_merging
        self.relative_pos = relative_pos
        
        norm_layer = partial(nn.LayerNorm, eps=1e-6)
        
        # final vit n layer for next cross attantion
        # here set 1
        self.blocks = nn.ModuleList([
            Block(
                dim=vision_width, num_heads=24 if vit !='large' else 32, mlp_ratio=4, qkv_bias=True, qk_scale=None,
                drop=0., attn_drop=0., drop_path=0., norm_layer=norm_layer,
                use_grad_checkpointing=use_grad_checkpointing)
            for i in range(1)])
        print('final vit depth {}'.format(1))
        
        if use_scale_swin:
            window_size = 4 if image_size == 224 else 6 # pretrain 4 finetune to set 6 or 8
            self.scale_swin = SwinTransformerScale(in_channels=vision_width,embed_dims=vision_width,
            window_size=window_size, strides=(window_size,), num_heads=(24,) if vit!='large' else (32,))
            print('use_scale_swin, window size {}'.format(window_size))
        elif use_scale_local:
            window_size = 4 if image_size == 224 else 6
            padding_mask = False
            print('use_scale_local, window size {}'.format(window_size))
            print('enable padding mask {}'.format(padding_mask))
            self.scale_local = LocalTransformerScale(img_size = image_size, window_size = window_size, padding_mask=padding_mask)
        elif use_scale_merging:
            kernel_size = 4 if image_size == 224 else 6
            self.scale_merging = SwinPatchMerging(vision_width, vision_width, kernel_size=kernel_size)
            print('use_scale_merging, kernel_size {}'.format(kernel_size))
            
        self.norm = norm_layer(vision_width)
        
        # init_weights
        self.init_weights()
        
    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                trunc_normal_init(m, std=.02, bias=0.)
            elif isinstance(m, nn.LayerNorm):
                constant_init(m, 1.0)
            elif isinstance(m, nn.Conv2d):
                trunc_normal_init(m, std=.02, bias=0.)
        print('init_weights() in Create_Vit_Scale')
        
    def forward(self, image):
        image_embeds = self.visual_encoder(image)
        
        B, _, C = image_embeds.shape # torch.rand(1, 197, 768)
        
        cls_tokens = image_embeds[:,0,:]
        cls_tokens = cls_tokens.view(B,-1,C)
        
        image_embeds = image_embeds[:,1:,:] # torch.rand(1, 196, 768)
        B, L, C = image_embeds.shape # torch.rand(1, 196, 768)

        h = int(math.sqrt(L))
        w = int(math.sqrt(L))
        assert h*w == L
        hw_shape = (h, w)
        
        if self.use_scale_swin:
            image_embeds_scale = self.scale_swin(image_embeds, hw_shape)[0] # swin based window atention and PatchMerging
        elif self.use_scale_local:
            image_embeds_scale = self.scale_local(image_embeds) # local vit layer for scale
        elif self.use_scale_merging:
            cls_tokens = image_embeds.mean(dim=1, keepdim=True)
            image_embeds_scale,_ = self.scale_merging(image_embeds, hw_shape)
            
        # same with 0907
        x = torch.cat((cls_tokens, image_embeds_scale, image_embeds), dim=1) # 
            
        # final vit one layer for next cross attantion
        
        for i, blk in enumerate(self.blocks):
            x = blk(x)
        x = self.norm(x)
        
        return x

def is_url(url_or_filename):
    parsed = urlparse(url_or_filename)
    return parsed.scheme in ("http", "https")

#
def set_freeze_by_names(model, layer_names, freeze=True):
    assert isinstance(layer_names, list)
    # print(layer_names)
    for name, child in model.named_modules():
        if name in layer_names:
            for param in child.parameters():
                param.requires_grad = not freeze
                # print('name: {}, grad" {}'.format(name, param.requires_grad))

def load_checkpoint(model,url_or_filename, freeze_layers_num = 0):
    if is_url(url_or_filename):
        cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)
        checkpoint = torch.load(cached_file, map_location='cpu') 
    elif os.path.isfile(url_or_filename):        
        checkpoint = torch.load(url_or_filename, map_location='cpu') 
    else:
        raise RuntimeError('checkpoint url or path is invalid')
        
    state_dict = checkpoint['model']
    
    if model.use_swin:
        # state_dict = change_checkpoint(model, state_dict)
    
        state_dict['visual_encoder.visual_encoder.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.visual_encoder.pos_embed'],model.visual_encoder.visual_encoder) 
        if 'visual_encoder.visual_encoder_m.pos_embed' in model.state_dict().keys():
            state_dict['visual_encoder.visual_encoder_m.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.visual_encoder_m.pos_embed'],
                                                                             model.visual_encoder.visual_encoder_m)
     
    else:
        state_dict['visual_encoder.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder) 
        if 'visual_encoder_m.pos_embed' in model.state_dict().keys():
            state_dict['visual_encoder_m.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],
                                                                             model.visual_encoder_m) 
    
    # print('state_dict {}'.format(list(state_dict.keys())[10:]))
    # print('model dict {}'.format(list(model.state_dict().keys())[:10]))
    
    for key in model.state_dict().keys():
        if key in state_dict.keys():
            if state_dict[key].shape!=model.state_dict()[key].shape:
                del state_dict[key]
    
    msg = model.load_state_dict(state_dict,strict=False)
    
    if freeze_layers_num > 0:
        freeze_layers = ['visual_encoder.patch_embed',]
        for i in range(freeze_layers_num):
            freeze_layers.append('visual_encoder.blocks.{}'.format(i))
            
        if model.use_swin:
            freeze_layers = ['visual_encoder.'+item for item in freeze_layers]

        set_freeze_by_names(model, freeze_layers)

    print('load checkpoint from %s'%url_or_filename)  
    return model,msg
    
    
def load_scale_pretrained(model, scale_pretrained = ''):
    # load pretrained with scale in pretraining checkpoint
    checkpoint = torch.load(scale_pretrained, map_location='cpu') 
    state_dict = checkpoint["model"]
    for k in list(state_dict.keys()):
        if k.startswith('text_encoder.'):
            state_dict[k.replace('text_encoder.','')] = state_dict[k]
        del state_dict[k]
    # print('remain keys :{}'.format(len(state_dict.keys())))
    msg = model.text_encoder.load_state_dict(state_dict,strict=False)
    print('text_encoder loadded from {}, missing keys {}'.format(scale_pretrained, len(msg.missing_keys)))
    return model, msg
    

def change_checkpoint(model, state_dict):
    temp = OrderedDict()
    for k in state_dict:
        k2 = k.replace('visual_encoder.', 'visual_encoder.visual_encoder.') if k.startswith('visual_encoder.') else k
        # k2 = k2.strip()
        temp[k2] = state_dict[k]
    return temp
    


            
